# LM Comparison Tool – Setup Guide

## Project Description

The **LM Comparison Tool** is a learning-focused application designed to compare responses from different Large Language Models (LLMs). It helps evaluate models based on **quality, accuracy, speed, and real‑world usefulness**.



## Environment Setup

### 1. Navigate to Project Folder

```bash
cd "LLM comparision"
```

### 2. Create a Virtual Environment

```bash
python -m venv venv
```

### 3. Activate the Virtual Environment

**Windows:**

```bash
venv\Scripts\activate
```

**Mac / Linux:**

```bash
source venv/bin/activate
```

### 4. Install Dependencies

```bash
pip install -r requirements.txt
```



## Run the Streamlit App

```bash
python -m streamlit run app.py
```

After running the command, open the **local URL** shown in the terminal in your browser.




## Project Aim

The goal of this project is to evaluate and compare different LLMs based on:

* Response accuracy
* Speed
* Quality of answers
* Usefulness for real‑world tasks




This project was created **for learning and academic purposes**.
